---
title: "Predicting Income with Census Data"
author: "Han Lu"
date: "6/16/2020"
output:
  pdf_document: default
  html_document: default
---

```{r global_options, include=FALSE}
library(tidyverse)
library(caret)
library(data.table)
library(gridExtra)
library(rpart)
library(caret)
library(randomForest)
library(tinytex)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

## I. Introduction
This project aims to predict an individual's income level with machine learning models using the 14 attributes provided by the census income dataset. Specifically, I implement a classification tree and random forest algorithm to predict whether a person's annual income is more than $50,000.

The "Census Income" dataset, also known as the "Adult" dataset, was extracted by Barry Becker from the 1994 US Census database for a prediction task on income level. The dataset consists of 48,842 observations and 15 variables, six numerical and nine categorical, and is archived at the UC Irvine Machine Learning Repository. The following is a list of variable names by their types.

|Categorical Variable  |Numerical Variable  |      
|----------------------|--------------------|
|income                |age                 |
|workclass             |fnlwgt              |
|education             |education.num       |
|occupation            |capital.gain        |
|marital.status        |capital.loss        |
|relationship          |hours.per.week      |
|race                  |                    |
|sex                   |                    |
|native.country        |                    |

The ```income``` variable identifies whether a person makes over $50,000 annually, and is used as the dependent variable. The dataset is divided into a ```train``` set with 32,561 observations, and a ```test``` set with 16,281 observations. After removing the missing values denoted by question marks, there are 45,222 rows left in the dataset with 30,162 rows in the ```train``` set and 15,060 rows in the ```test``` set.

The variable names are mostly self-explanatory, with a few exceptions. The variable ```education``` is a categorical representation of the numerical ```education.num```, which stands for the number of years of education. The variable ```relationship``` indicates the respondent's role in the family. The variable ```fnlwgt``` specifies the final weight, which is the number of people the census believes the entry represents. Gain and loss from investments are stored in ```capital.gain``` and ```capital.loss```. The variable ```hours.per.week``` indicates the number of working hours per week.

```{r prepare, echo = TRUE}
# Download the Adult Dataset (Census Income Dataset)

# Census Income Data Set:
# https://archive.ics.uci.edu/ml/datasets/Adult

url_train <- 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'
url_test <- 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test' 
url_names <- 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names' 

train <- read.table(url_train, sep = ',', stringsAsFactors = FALSE)
test <- readLines(url_test)[-1]
test <- read.table(textConnection(test), sep = ',', stringsAsFactors = FALSE)

names <- readLines(url_names)[97:110]
names <- as.character(lapply(strsplit(names,':'), function(x) x[1])) 
names <- c(names, 'income')
colnames(train) <- names
colnames(test) <- names

# Remove the missing values (denoted by question marks)

no.question.mark <- apply(train, 1, function(r) !any(r %in% ' ?'))
train <- train[no.question.mark,]
no.question.mark <- apply(test, 1, function(r) !any(r %in% ' ?')) 
test <- test[no.question.mark,]

train <- as.data.frame(unclass(train),stringsAsFactors = T) 
test <- as.data.frame(unclass(test),stringsAsFactors = T)

# Create adult set (combine train, test set)

adult <- rbind(train, test)

# Remove the "." from "<=50K." and ">50K." in adult set

adult$income <- gsub(".", "", as.character(adult$income), fixed = TRUE)

str(adult)
```

```{r train, echo = TRUE}
dim(train)
```

```{r test, echo = TRUE}
dim(test)
```

## II. Analysis
### 2.1 Data Exploration
From the histograms of the numerical variables, I find that the distributions of ```capital.gain``` and ```capital.loss``` are very narrow and concentrated at zero. The zeros constitute 91.61912% of ```capital.gain``` entries and 95.26779% of ```capital.loss``` entries. In the modeling section, I combine these two columns into a single variable called ```capital.change``` for more efficient representation.

```{r numeric, echo = TRUE}
# Visualize numerical variables

# Histogram of Age

p1 <- ggplot(train, aes(x = age)) + 
  ggtitle("Histogram of Age") + 
  geom_histogram(
    aes(y = 100*(..count..)/sum(..count..)), 
    binwidth = 5, colour = "black", fill = "#F0E442"
    ) + 
  ylab("Percentage") +  
  theme_minimal()

# Histogram of Final Weight

p2 <- ggplot(adult, aes(x = log10(fnlwgt))) +
  ggtitle("Histogram of Final Weight") + 
  geom_histogram(
    aes(y = 100*(..count..)/sum(..count..)), 
    colour = "black", fill = "#F0E442"
    ) + 
  ylab("Percentage") +
  theme_minimal()

# Histogram of Years of Education

p3 <- ggplot(adult, aes(x = education.num)) + 
  ggtitle("Histogram of Years of Education") + 
  geom_histogram(
    aes(y = 100*(..count..)/sum(..count..)), 
    binwidth = 1, colour = "black", fill = "#F0E442"
    ) + 
  ylab("Percentage") +
  theme_minimal()

# Histogram of Hours per Week

p4 <- ggplot(adult, aes(x = hours.per.week)) + 
  ggtitle("Histogram of Hours per Week") + 
  geom_histogram(
    aes(y = 100*(..count..)/sum(..count..)), 
    colour = "black", fill = "#F0E442"
    ) + 
  ylab("Percentage") +
  theme_minimal()

# Histogram of Capital Gain

p5 <- ggplot(adult, aes(x = log10(capital.gain+1))) +
  ggtitle("Histogram of Capital Gain") + 
  geom_histogram(
    aes(y = 100*(..count..)/sum(..count..)), 
    colour = "black", fill = "#F0E442"
    ) + 
  ylab("Percentage") +
  theme_minimal()

# Histogram of Capital Loss

p6 <- ggplot(adult, aes(x = log10(capital.loss+1))) + 
  ggtitle("Histogram of Capital Loss") + 
  geom_histogram(
    aes(y = 100*(..count..)/sum(..count..)), 
    colour = "black", fill = "#F0E442"
    ) + 
  ylab("Percentage") +
  theme_minimal()

grid.arrange(p1,p2,p3,p4,p5,p6)
```

```{r gain, echo = TRUE}
# Percentage of data with zero capital gain

sum(adult$capital.gain==0)/length(adult$capital.gain)*100
```

```{r loss, echo = TRUE}
# Percentage of data with zero capital loss

sum(adult$capital.loss==0)/length(adult$capital.loss)*100
```

The histograms of the categorical variables are plotted as follows.

```{r categorical, echo = TRUE}
# Sort categorical variables in descending order

sort.categ <- function(x){reorder(x,x,function(y){length(y)})}
var.categ <- which(sapply(adult, is.factor))
for (c in var.categ){adult[,c] <- sort.categ(adult[,c])}
attach(adult)

# Histogram of Work Class

ggplot(adult, aes(y = workclass)) + 
  ggtitle("Histogram of Work Class") + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), colour = "black", fill = "#F0E442") + 
  scale_y_discrete(limits = levels(workclass)) +
  xlab("Percentage") +
  ylab("Work Class") +
  theme_minimal()

# Histogram of Education

ggplot(adult, aes(y = education)) + 
  ggtitle("Histogram of Education") + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), colour = "black", fill = "#F0E442") + 
  scale_y_discrete(limits = levels(education)) +
  xlab("Percentage") +
  ylab("Education") +
  theme_minimal()

# Histogram of Marital Status

ggplot(adult, aes(y = marital.status)) + 
  ggtitle("Histogram of Marital Status") + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), colour = "black", fill = "#F0E442") + 
  scale_y_discrete(limits = levels(marital.status)) +
  xlab("Percentage") +
  ylab("Marital Status") +
  theme_minimal()

# Histogram of Occupation

ggplot(adult, aes(y = occupation)) + 
  ggtitle("Histogram of Occupation") + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), colour = "black", fill = "#F0E442") + 
  scale_y_discrete(limits = levels(occupation)) +
  xlab("Percentage") +
  ylab("Occupation") +
  theme_minimal()

# Histogram of Relationship

ggplot(adult, aes(y = relationship)) + 
  ggtitle("Histogram of Relationship") + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), colour = "black", fill = "#F0E442") + 
  scale_y_discrete(limits = levels(relationship)) +
  xlab("Percentage") +
  ylab("Relationship") +
  theme_minimal()

# Histogram of Race

ggplot(adult, aes(y = race)) + 
  ggtitle("Histogram of Race") + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), colour = "black", fill = "#F0E442") + 
  scale_y_discrete(limits = levels(race)) +
  xlab("Percentage") + 
  ylab("Race") +
  theme_minimal()

# Histogram of Sex

ggplot(adult, aes(y = sex)) + 
  ggtitle("Histogram of Sex") + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), colour = "black", fill = "#F0E442") + 
  scale_y_discrete(limits = levels(sex)) +
  xlab("Percentage") +   
  ylab("Sex") +
  theme_minimal()

# Histogram of Native Country

ggplot(adult, aes(y = native.country)) + 
  ggtitle("Histogram of Native Country") + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), colour = "black", fill = "#F0E442") + 
  scale_y_discrete(limits = levels(native.country)) +
  xlab("Percentage") +
  ylab("Native Country") +
  theme_minimal()

```

The variable ```native.country``` shows a narrow distribution with 91.30954% of the respondents coming from the United States.

```{r native, echo = TRUE}
summary(adult$native.country)
```

To identify the potential variable that impacts the income level, I plot the attributes against income to examine correlations. The boxplots show that the numeric variables ```age```, ```education```, and ```hours.per.week``` are correlated with income.

```{r boxplot, echo = TRUE}
# Final weight and income

b1 <- ggplot(adult, aes(income, log(fnlwgt))) +
  geom_boxplot(coef=3) +
  xlab("Income") +
  ylab("Final Weight") +
  theme_minimal()
  
# Age and income

b2 <- ggplot(adult, aes(income, age)) +
  geom_boxplot(coef=3) +
  xlab("Income") +
  ylab("Age") +
  theme_minimal()

# Education and income

b3 <- ggplot(adult, aes(income, education.num)) +
  geom_boxplot(coef=3) +
  xlab("Income") +
  ylab("Years of Education") +
  theme_minimal()

# Hours per week and income

b4 <- ggplot(adult, aes(income, hours.per.week)) +
  geom_boxplot(coef=3) +
  xlab("Income") +
  ylab("Hours per Week") +
  theme_minimal()
  
grid.arrange(b1, b2, b3, b4)
```

I use bar charts to show the correlation between categorical variables and income.

```{r workclass, echo = TRUE}

# Work class and income

ggplot(adult, aes(y = workclass, fill = income)) + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), position='dodge') +
  ggtitle("Work Class and Income") +
  labs(fill = "Income") +
  xlab("Percentage") +
  ylab("Work Class") +
  theme_minimal()
```

```{r occupation, echo = TRUE}
# Occupation and income

ggplot(adult, aes(y = occupation, fill = income)) + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), position='dodge') +
  ggtitle("Occupation and Income") +
  labs(fill = "Income") +
  xlab("Percentage") +
  ylab("Occupation") +
  theme_minimal()
```

```{r education, echo = TRUE}
# Education and income

ggplot(adult, aes(y = education, fill = income)) + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), position='dodge') +
  ggtitle("Education and Income") +
  labs(fill = "Income") +
  xlab("Percentage") +
  ylab("Education") +
  theme_minimal()
```

```{r marriage, echo = TRUE}
# Marital status and income

ggplot(adult, aes(y = marital.status, fill = income)) + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), position='dodge') +
  ggtitle("Marital Status and Income") +
  labs(fill = "Income") +
  xlab("Percentage") +
  ylab("Marital Status") +
  theme_minimal()
```

```{r relationship, echo = TRUE}
# Relationship and income

ggplot(adult, aes(y = relationship, fill = income)) + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), position='dodge') +
  ggtitle("Relationship and Income") +
  labs(fill = "Income") +
  xlab("Percentage") +
  ylab("Relationship") +
  theme_minimal()
```

```{r race, echo = TRUE}
# Race and income

ggplot(adult, aes(y = race, fill = income)) + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), position='dodge') +
  ggtitle("Race and Income") +
  labs(fill = "Income") +
  xlab("Percentage") +
  ylab("Race") +
  theme_minimal()
```

```{r sex, echo = TRUE}
# Sex and income

ggplot(adult, aes(y = sex, fill = income)) + 
  geom_bar(aes(x = 100*(..count..)/sum(..count..)), position='dodge') +
  ggtitle("Sex and Income") +
  labs(fill = "Income") +
  xlab("Percentage") +
  ylab("Sex") +
  theme_minimal()
```

As we see from the above graphs, all categorical variables show an influence on income level.

### 2.2 Data Cleaning
This section presents the data cleaning process for later modeling.

```{r cleaning, echo = TRUE}
# Combine capital.gain and capital.loss into capital.change

train$capital.change <- train$capital.gain - train$capital.loss
test$capital.change <- test$capital.gain - test$capital.loss

train$capital.gain <- NULL
train$capital.loss <- NULL
test$capital.gain <- NULL
test$capital.loss<-NULL

# Switch income and capital.change columns (let income be the last column)

train[c(11,12)] <- train[c(12,11)]
colnames(train)[11:12] <- colnames(train)[12:11]
test[c(11,12)] <- test[c(12,11)]
colnames(test)[11:12] <- colnames(test)[12:11]


# Delete education variable in train and test set

train$education <- NULL
test$education <- NULL

# Delete native.country variable in train and test set

train$native.country <- NULL
test$native.country <- NULL

# Convert income to dummy variable

train$income <- as.factor(ifelse(train$income == ' <=50K', 0, 1))
test$income <- as.factor(ifelse(test$income == ' <=50K.', 0, 1))
```

### 2.3 Classification Tree
In the classification tree model, we achieve a prediction accuracy of 83.89774%.

```{r tree, echo = TRUE}
set.seed(1, sample.kind = "Rounding")
tree <- rpart(income ~ ., data = train, method = 'class')
tree.hat <- predict(tree, newdata = test, type = 'class')
confusionMatrix(tree.hat, test$income)$overall["Accuracy"]
```

### 2.3 Random Forest
We achieve the OOB error rate of 13.92% and prediction accuracy of 85.73705% with the random forest model. The top five variables that appear to be most important are ```capital.change```, ```fnlwgt```, ```age```, ```relationship```, and ```education.num```, as shown in the variable importance plot.

```{r forest, echo = TRUE}
set.seed(1, sample.kind="Rounding")
forest <- randomForest(train$income ~ ., data = train, mtry = sqrt(10), importance = TRUE) 
forest
```

```{r accuracy, echo = TRUE}
forest.hat <- predict(forest, newdata = test, type = "class") 
confusionMatrix(forest.hat, test$income)$overall["Accuracy"]
```

```{r importance, echo = TRUE}
varImpPlot(forest, type = 2, main = "Variable Importance Plot")
```

## III. Conclusion
In this project, I built a classification tree and random forest models to predict a person's income level with the Census Income dataset. The random forest model achieved an accuracy of 85.73705%, higher than the 83.89774% provided by the initial classification tree model. A significant portion of the income variability is generated by the investment gain and loss, the final weight assigned by the census, age, role in the family, and education level. The future work would focus on building other machine learning models such as logistic regression, KNN, Naive Bayes, and neural network to increase the accuracy and predictive power.
