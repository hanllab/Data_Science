---
title: "MovieLens Project Report"
subtitle: 'Building a Movie Recommendation System'
author: "Han Lu"
date: "6/6/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org")

knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

## I. Introduction
The recommendation system is an integral part of today's e-commerce and online services industry. It helps the user find the best products and services that suit their needs based on their previous activity. Companies such as Netflix, YouTube, and Amazon rely on these systems to provide their customers personalized experience and increase their satisfaction. For example, in 2006, Netflix offered a one million dollar prize to improve at least 10% of its recommendation system.

The goal of this project is to build a movie recommendation system using the MovieLens dataset. The remainder of the project is organized as follows. After a brief review of the dataset and model evaluation methods in part I, part II presents the analysis and results. This includes data preparation, data exploration, and modeling. The modeling segment consists of linear modeling, regularization, and final model evaluation. Finally, part III concludes the project and provides a summary of the results.

### 1.1 Dataset
Collected by GroupLens, a research lab at the University of Minnesota, the full MovieLens dataset consists of 27 million ratings and 1.1 million tag applications applied to 58,000 movies by 280,000 users. This project uses the MovieLens 10M Dataset, a stable benchmark subset of the full dataset that provides 10 million ratings and 100,000 tag applications applied to 10,000 movies by 72,000 users.

### 1.2 Evaluation Method
A general approach to evaluate machine learning algorithms is to define a loss function that measures the difference between the predicted value and observed outcome. Since lower loss produces higher accuracy, our goal is to minimize the loss, so it is as close to zero as possible. Here we use three commonly used loss functions: mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE). All three metrics are computed and reported in this exercise for comparison, although we only focus on minimizing the RMSE when choosing the best algorithm since it is in the same units as the outcomes. The formulas of these metrics are
$$MAE=\frac{1}{N}\sum_{i=1}^{N} |\hat{y_i}-y_i|$$
$$MSE=\frac{1}{N}\sum_{i=1}^{N} (\hat{y_i}-y_i)$$
$$RMSE=\sqrt{\frac{1}{N}\sum_{i=1}^{N} (\hat{y_i}-y_i)}$$
where $\hat{y_i}$ is the predicted value, $y$ is the observed outcome, and $N$ is the number of observations. RMSE is the square root of MSE. When the outcomes are binary, both metrics are equivalent since $(\hat{y}-y)^2$ is zero if the prediction was correct and one otherwise. Unlike RMSE and MSE, which use squared loss, MAE uses absolute values instead. We can define the loss functions with the following code.

```{r loss_functions, echo=TRUE}
# Define Mean Absolute Error (MAE)
MAE <- function(true_ratings, predicted_ratings){
  mean(abs(true_ratings - predicted_ratings))
}

# Define Mean Squared Error (MSE)
MSE <- function(true_ratings, predicted_ratings){
  mean((true_ratings - predicted_ratings)^2)
}

# Define Root Mean Squared Error (RMSE)
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

## II. Analysis and Results
This section presents the data preparation, data exploration, and modeling procedure.

### 2.1 Data Preparation
We download the MovieLens 10M Dataset and split it into a study set (```edx```) and a validation set (```validation```) with a 90/10 split percentage ratio. The ```edx``` set is then split again into a training set and a test set with the same split ratio for the modeling process. The ```validation``` set is only used for evaluating the final algorithm. For the final test of the algorithm, we predict movie ratings in the ```validation``` set as if they are unknown to us, and use RMSE to determine how close the final predictions are to the true values.

```{r data_prep, echo=TRUE}
# Create edx set, validation set

# Note: this process could take a couple of minutes

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

```{r edx_split, echo=TRUE}
# Train-test split of the edx set

set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set

test <- temp %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")

# Add rows removed from test set back into train set

removed <- anti_join(temp, test)
train <- rbind(train, removed)

rm(test_index, temp, removed)
```

### 2.2 Data Exploration
The ```edx``` set has 9,000,055 entries with six variables or columns. Similarly, the ```validation``` set has 999,999 entries and six columns. The dataset is in a tidy format, which gives us one observation of each row and columns as variables. Below is a summary of the ```edx``` set.

```{r glimpse_edx, echo=TRUE}
glimpse(edx)
```

```{r summary_edx, echo=TRUE}
summary(edx)
```

The ```userId``` variable identifies the user information, while the ```movieId``` and ```title``` columns identify the movie information. Each movie is tagged with one or more genres, as shown in the ```genres``` column. Movie ratings are stored in the ```rating``` column. The variable ```timestamp``` contains the rating dates measured in seconds with January 1st, 1970, as the epoch.

Now we take a closer look at each variable. The ```timestamp``` variable indicates that the data was collected over almost 14 years.

```{r date, echo=TRUE}
# Summarize the dates of the dataset

tibble('Start Date' = date(as_datetime(min(edx$timestamp), origin="1970-01-01")),
       'End Date' = date(as_datetime(max(edx$timestamp), origin="1970-01-01"))) %>%
  mutate(Span = duration(max(edx$timestamp)-min(edx$timestamp)))
```

The ```userId``` and ```movieId``` columns show that 69,878 unique users are rating 10,677 different movies, indicating that the number of ratings varies. Some of the movies are more popular and rated more than others. We can see this pattern clearly from the movies' distribution by the number of ratings.

```{r users, echo=TRUE}
# Number of users in the edx set

length(unique(edx$userId))
```

```{r movies, echo=TRUE}
# Number of movies in the edx set

length(unique(edx$movieId))
```

```{r movies_dist, echo=TRUE}
# Distribution of movies by the number of ratings

edx %>% group_by(movieId) %>%
  summarise(n=n()) %>%
  ggplot(aes(n)) +
    geom_histogram(color = "white") +
    scale_x_log10() + 
    ggtitle("Distribution of Movies by the Number of Ratings", 
            subtitle = "Some movies get rated more than others.") +
    xlab("Number of Ratings") +
    ylab("Number of Movies") +
    theme_economist()
```

Some users are also more active than others, as suggested in the users' distribution below.

```{r user_dist, echo=TRUE}
# Distribution of users by the number of ratings

edx %>% group_by(userId) %>%
  summarise(n=n()) %>%
  ggplot(aes(n)) +
    geom_histogram(color = "white") +
    scale_x_log10() + 
    ggtitle("Distribution of Users by the Number of Ratings", 
            subtitle="Some users are more active than others.") +
    xlab("Number of Ratings") +
    ylab("Number of Users") + 
    scale_y_continuous(labels = comma) + 
    theme_economist()
```

The following user-movie sparse matrix presents a random sample of 100 movies and 100 users with yellow indicating a rating that exists for that user-movie combination. The matrix is sparse with the majority of empty cells. This makes it easy for us to identify that some movies get more ratings, and some users are more active than others.

```{r sparse, echo=TRUE}
# User-movie sparse matrix

users <- sample(unique(edx$userId), 100)
edx %>% filter(userId %in% users) %>%
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% 
  select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
title("User-Movie Matrix")
```

The data also shows, as expected, that the well-known blockbusters tend to have the highest number of ratings, with *Pulp Fiction* (1994) ranked on top of the list with 31,362 ratings in total.

```{r most_rated, echo=TRUE}
# Most rated movies

edx %>% group_by(title) %>%
  summarize(n= n()) %>%
  arrange(desc(n))
```

Meanwhile, there are more than 100 movies with only a single rating.

```{r rated_once, echo=TRUE}
# Number of movies rated once

edx %>% group_by(title) %>%
  summarize(n = n()) %>%
  filter(n==1) %>%
  count() %>%
  pull()
```

When rating movies, users can choose to give one of the ten rating points ranging from 0.5 to 5. The distribution of ratings indicates that the five most given ratings in order from most to least are 4, 3, 5, 3.5, and 2. In general, half-star ratings are less common than whole star ratings. For instance, there are fewer ratings of 3.5 than there are ratings of 3 or 4. This indicates that most users tend to round decimal scores to integers when giving ratings.

```{r ratings_dist, echo=TRUE}
# Count the number of each rating

edx %>% group_by(rating) %>% summarize(n=n())
```

Finally, there are 797 different combinations of genres.

```{r genres, echo=TRUE}
length(unique(edx$genres))
```

### 2.3 Modeling
This segment presents linear modeling, regularization, and final model evaluation.

#### A. Linear Model
The simplest model is to use the average rating across all users and movies for prediction. This method assumes that the rating variation is entirely from the randomly distributed error term, as shown in the formula below.
$$\hat{Y_{u,i}}=\mu+\epsilon_{u,i}$$
The $\hat{Y_{u,i}}$ is the predicted rating of user $u$ and movie $i$, $\mu$ is the mean rating from the observed data, and $\epsilon$ is the error term. The code and results of this simple model are shown as follows.

```{r mean_model, echo=TRUE}
# Mean of the observed values in the training set

mu <- mean(train$rating)

# Report results

result <- bind_rows(tibble(Method = "Mean", 
                           RMSE = RMSE(test$rating, mu),
                           MSE  = MSE(test$rating, mu),
                           MAE  = MAE(test$rating, mu)))
result
```

Since we see in the data exploration section that movies differ in popularity, therefore it is reasonable to add a movie effect term, $b_i$, that count this movie to movie variability to our mean model. The movie effect term can be defined as the difference between the observed and the mean rating. The model can be written as
$$\hat{Y_{u,i}}=\mu+b_i+\epsilon_{u,i}$$
where
$$\hat{b_i}=\frac{1}{N}\sum_{i=1}^{N} (y_i-\mu).$$
As we can see in the results, this change brings improvement to the RMSE.

```{r movie_effect, echo=TRUE}
# Create movie effect term (b_i)

b_i <- train %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
head(b_i)

# Predict ratings with mu and b_i

y_hat <- test %>%
  left_join(b_i, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)

# Report results

result <- bind_rows(result, 
                    tibble(Method = "Movie Effect", 
                           RMSE = RMSE(test$rating, y_hat),
                           MSE  = MSE(test$rating, y_hat),
                           MAE  = MAE(test$rating, y_hat)))
result
```

Like the movie effect, the user to user variability can also be modeled by adding a user effect term $\hat{b_u}$, which captures the users' rating patterns. The model can be written as
$$\hat{Y_{u,i}}=\mu+b_i+b_u+\epsilon_{u,i}$$
where
$$\hat{b_u}=\frac{1}{N}\sum_{i=1}^{N} (y_{u,i}-b_i-\mu).$$
This model further improves the RMSE.

```{r movie_user_effect, echo=TRUE}
# Create user effect term (b_u)

b_u <- train %>% 
  left_join(b_i, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

# Predict ratings with mu, b_i, and b_u

y_hat <- test %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

# Update results

result <- bind_rows(result, 
                    tibble(Method = "Movie and User Effect", 
                           RMSE = RMSE(test$rating, y_hat),
                           MSE  = MSE(test$rating, y_hat),
                           MAE  = MAE(test$rating, y_hat)))
result
```

#### B. Regularization
As we see during the data exploration process, many movies and users have a very low number of ratings. This leads to a large estimated error due to the small sample size. We employ regularization, penalizing small sample sizes to reduce the effect of error and controlling the total variability of the movie and user effects in our estimation. Instead of minimizing the least-squares equation, we minimize
$$\frac{1}{N}\sum_{u,i} (y_{u,i}-\mu-b_i-b_u)^2+\lambda(\sum_{i}b_i^2+\sum_{u}b_u^2)$$
where the first term is our previous least squares equation, and the last term is the penalty for large values of $b_i$ and $b_u$. The equations of $b_i$ and $b_u$ corresponding to this minimization are
$$\hat{b_i}=\frac{1}{\lambda+n_i}\sum_{u=1}^{n_i} (y_{u,i}-\hat{\mu})$$
$$\hat{b_u}=\frac{1}{\lambda+n_u}\sum_{i=1}^{n_u} (y_{u,i}-\hat{\mu}-\hat{b_i}).$$
When $n_i$ is the number of ratings made for movie $i$, $n_u$ is the number of ratings made by user $u$. When the sample size is very large, we have $n_i+\lambda\approx n_i$, and the penalty $\lambda$ is effectively ignored. However, when the sample size is small, $\hat{b_i}$ and $\hat{b_u}$ are shrunken toward 0 by the scale of $\lambda$. Since $\lambda$ is a tuning parameter, we use cross-validation to choose the one that minimizes RMSE.

```{r lambda, echo=TRUE}
# Define lambdas

lambdas <- seq(from=0, to=10, by=0.25)

# Compute RMSE for each lambda

rmses <- sapply(lambdas, function(l){
  
  # Mean rating
  
  mu <- mean(train$rating)
  
  # Movie effect (b_i)
  
  b_i <- train %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  # User effect (b_u)
  
  b_u <- train %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  # Predictions from y_hat = mu + b_i + b_u
  
  predicted_ratings <- test %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  # Report RMSE
  
  return(RMSE(predicted_ratings, test$rating))
})

# Plot the lambda vs RMSE

tibble(Lambda = lambdas, RMSE = rmses) %>%
  ggplot(aes(x = Lambda, y = RMSE)) +
  geom_point() +
  ggtitle("Regularization", 
          subtitle = "The optimal penalization gives the lowest RMSE.") +
  theme_economist()

# Define the optimal lambda

lambda <- lambdas[which.min(rmses)]
```

Now we are ready to estimate the regularized model with the optimal $\lambda$.

```{r regularization, echo=TRUE}
# Regularized movie effect (b_i)

b_i <- train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

# Regularized user effect (b_u)

b_u <- train %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

# Prediction

y_hat <- test %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

# Report results

result <- bind_rows(result, 
                    tibble(Method = "Regularized Movie and User Effect", 
                           RMSE = RMSE(test$rating, y_hat),
                           MSE  = MSE(test$rating, y_hat),
                           MAE  = MAE(test$rating, y_hat)))
result
```

The RMSE in the regularized model is slightly lower than the previous estimations.

#### C. Model Evaluation
We proceed to the final evaluation of this model since the regularized movie and user effect model produces the lowest RMSE. We train the model again, use the entire ```edx``` set, and use the ```validation``` set to check the performance.

```{r final_validation, echo=TRUE}
# Mean rating

mu <- mean(edx$rating)

# Regularized movie effect (b_i)

b_i <- edx %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

# Regularized user effect (b_u)

b_u <- edx %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

# Prediction

y_hat <- validation %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

# Report results

result <- bind_rows(result, 
                    tibble(Method = "Final Validation", 
                           RMSE = RMSE(validation$rating, y_hat),
                           MSE  = MSE(validation$rating, y_hat),
                           MAE  = MAE(validation$rating, y_hat)))
result
```

The RMSE computed using the ```validation``` set is similar to what we have achieved during the model building process, confirming the model's performance. The top 10 best and top 10 worst movies predicted by this algorithm are shown below.

Top 10 Best Movies:
```{r 10_best, echo=TRUE}
# Top 10 best movies

validation %>% 
left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>% 
  mutate(pred = mu + b_i + b_u) %>% 
  arrange(-pred) %>% 
  group_by(title) %>% 
  select(title) %>%
  head(10)
```

Top 10 Worst Movies:
```{r 10_worst, echo=TRUE}
# Top 10 worst movies

validation %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>% 
  mutate(pred = mu + b_i + b_u) %>% 
  arrange(pred) %>% 
  group_by(title) %>% 
  select(title) %>%
  head(10)
```

## III. Conclusion
In this project, we built a linear model that predicts movie ratings. After collecting, preparing, and exploring the dataset, we started with a simple mean model, just the observed ratings average. We then added the movie effect and user effect to the model to capture the variability caused by the movie's popularity and user's rating behavior. The regularization process reduced the estimation error generated by the movies and users with very few ratings. Our final model achieved the RMSE of 0.865. The table below is a summary of the results.

| Model                              | RMSE     | MSE      | MAE      |
|------------------------------------|----------|----------|----------|
| Mean                               | 1.06     | 1.12     |0.855     |
| Movie Effect                       | 0.943    | 0.889    |0.737     |
| Movie and User Effect              | 0.865    | 0.748    |0.668     |
| Regularized Movie and User Effect  | 0.864    | 0.747    |0.669     |
| Final Validation                   | 0.865    | 0.748    |0.669     |
